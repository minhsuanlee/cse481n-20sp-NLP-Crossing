# Blog Post 1 - Due April 7th

### Team Name: NLP Crossing

### Team Members: Sam Lee, Shuheng Liu, Robin Yang

### GitHub URL: https://github.com/minhsuanlee/cse481n-20sp-NLP-Crossing

### Top Three Project Ideas:

1. **Chatbot Interviewer**
    - Minimal Viable Plan:
      - Find a working chatbot that we can customize
      - Find suitable dataset to train and evaluate our model
      - Improve the chatbot to ask questions based on user's response
      - Summarize the user's response at the end of a conversation
    - Stretch Goals:
      - Change topic when no questions can be asked about the prev topic
      - Improve quality of question/summary
      - What if the user gives unrelated responses
    - Relevant Links:
      - [Incorporating Copying Mechanism in Sequence-to-Sequence Learning](https://arxiv.org/pdf/1603.06393.pdf)
      - [A Copy-Augmented Sequence-to-Sequence Architecture Gives Good Performance on Task-Oriented Dialogue](https://nlp.stanford.edu/pubs/meric2017copy.pdf)
      - [Possible Sources of Datasets & Models](http://nlpprogress.com/english/dialogue.html)
&nbsp;

2. **Research Paper Summarizer**
    - Minimal Viable Plan:
      - Find a summarization model
      - Generate a paper summary/abstract given a research paper (without abstract)
      - Evaluate the quality of the generated abstract
    - Stretch Goals:
      - Improve the quality of the generated abstract
      - Include linguistic features to make generation more fluent
    - Relevant Link:
      - [Neural Text Summarization: A Critical Evaluation](https://www.aclweb.org/anthology/D19-1051.pdf)
&nbsp;

3. **Discover What Else Can BERT Do**
    - Minimal Viable Plan:
      - Search for papers that applied BERT to different domains
      - Learn from those papers and apply BERT to other domain(s) of interest
    - Stretch Goal:
      - Analyze advantages and disadvantages of applying BERT to distinct domains as well as its performances
    - Relevant Links:
      - [What Does BERT Look At? An Analysis of BERTâ€™s Attention](https://arxiv.org/pdf/1906.04341.pdf)
      - [BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
      - [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf)
