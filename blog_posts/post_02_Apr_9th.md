# Blog Post 2 - Due April 9th

### Team Name: NLP Crossing

### Team Members: Sam Lee, Shuheng Liu, Robin Yang

__GitHub URL: https://github.com/minhsuanlee/cse481n-20sp-NLP-Crossing__

### Top Three Project Ideas:

1. **Chatbot Interviewer**
    - Pros:
        - High excitement among the team
        - Helpful for interviewees to elaborate on their own experiences
        - Have enough resources to succeed
    - Cons:
        - Might be challenging to find data of dialogues
    - Codebases/Platforms:
      - [Hugging Face codebase for Chatbot](https://github.com/huggingface/pytorch-openai-transformer-lm)
      - [The Conversational Intelligence Challenge 2 (ConvAI2)](http://convai.io/)
      - [AllenNLP CopyNet Codebase](https://github.com/allenai/allennlp/pull/2237/files/8404e006e10ca37d15fb75af578df898c6222bfc)
      - [AllenNLP CopyNet Documentation](https://docs.allennlp.org/master/api/models/encoder_decoders/copynet_seq2seq/)
      - [NL2Bash](https://github.com/TellinaTool/nl2bash)

2. **Research Paper Summarizer**
    - Pros:
        - Could find many similar experiments online.
        - Help us understand how BERT works internally
    - Cons:
        - Moderate excitement among the team
        - If re-training may be required after some modification to the codebase, there may not be sufficient computing resource available
    - Codebases/Platforms:
      - [HTML and Text Summarizer](https://github.com/miso-belica/sumy)
      - [Seq2seq News Summarizer](https://github.com/ymfa/seq2seq-summarizer)

3. **Discover What Else Can BERT Do**
    - Pros:
        - If succeed and is efficient, could help researchers write abstract
        - Find out if the model can capture what is important in a long corpus
    - Cons:
        - Moderate excitement among the team
        - Difficult to find previous models that work on similar problems
    - Codebases/Platforms:
      - [BERT Codebase](https://github.com/google-research/bert)
      - [RoBERTa Codebase](https://github.com/pytorch/fairseq/tree/master/examples/roberta)
      - [BERT Tutorial by Nvidia](https://nvidia.github.io/NeMo/nlp/bert_pretraining.html)
      - [BioBERT: a pre-trained biomedical language representation model for biomedical text mining](https://arxiv.org/pdf/1901.08746.pdf)

### Topic for Lecture or Class Discussion
- How does BERT work?
- How to design experiments based on previous papers?
- What are the common approaches to analyze NLP models?
- How to combine voice recognition with various NLP models? 
